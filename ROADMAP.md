# ğŸ› ï¸ DevMind Development Roadmap

A modular, LLM-powered CLI agent that helps developers understand and manipulate their code, logs, and environments.


## âœ… Phase 1: Core CLI + LLM Integration

### ğŸ¯ Goal: Standalone CLI that can talk to an LLM with basic prompt

- [ ] Setup `main.go` with CLI entrypoint (use `cobra` or `urfave/cli`)
- [ ] Implement `llm/` package for OpenAI API integration
- [ ] Read prompt from CLI flag or stdin
- [ ] Send prompt to OpenAI and print result
- [ ] Add `.env` config loader (`viper`, `godotenv`)


## ğŸš§ Phase 2: Context Ingestion Layer

### ğŸ¯ Goal: Add structured local context as input to LLM

- [ ] Parse Go/Python/JS files and extract:
  - [ ] Function names
  - [ ] Comments and signatures
- [ ] Parse Git diff (`git diff HEAD`)
- [ ] Sanitize and include environment variables
- [ ] Chunk large input into ~2K token-safe segments
- [ ] Compose final prompt with system + context + user input


## ğŸ§ª Phase 3: Feature Commands

### ğŸ¯ Goal: Build meaningful commands around DevMind capabilities

- [ ] `devmind explain <file>` â†’ explain a code file
- [ ] `devmind fix <logfile>` â†’ analyze and suggest fix
- [ ] `devmind run '<natural task>'` â†’ generate bash/Dockerfile/etc
- [ ] `devmind summary` â†’ summarize current Git diff + files


## ğŸŒ Phase 4: Multi-Model Support

### ğŸ¯ Goal: Run with different LLM backends

- [ ] Add Ollama integration via `http://localhost:11434/api/generate`
- [ ] Add backend selector flag: `--provider=openai|ollama`
- [ ] Normalize prompt/response handling across providers


## ğŸ’» Phase 5: Interactive REPL / Chat Mode

### ğŸ¯ Goal: Run DevMind in conversational mode

- [ ] Implement `devmind chat` REPL
- [ ] Persist context across messages (simple in-memory memory)
- [ ] Keyboard shortcuts for history, clearing, saving


## ğŸ“Š Phase 6: Advanced UX and Plugins

- [ ] Prompt templates (stored in `.devmind/templates/*.tmpl`)
- [ ] Syntax-highlighted output (color terminal / TUI)
- [ ] Plugin support: user-defined commands via YAML + Go templates
- [ ] LLM-based code linting


## ğŸ³ Phase 7: Deployment & Packaging

- [ ] Dockerize CLI for isolated use
- [ ] Provide binary releases via `goreleaser`
- [ ] Publish GitHub release + installation instructions


## âœ¨ Stretch Goals

- [ ] Web UI (Tauri/Webview or simple browser)
- [ ] VSCode Extension
- [ ] Token budget estimator and cost display
- [ ] Local model selector via `gguf`, llama.cpp, LM Studio
- [ ] Context caching for faster re-queries


## ğŸ”„ Milestone Summary

| Milestone        | Description                          | Status  |
|------------------|--------------------------------------|---------|
| âœ… Phase 1        | CLI + LLM Integration                | â³       |
| âœ… Phase 2        | Context gathering                    | â³       |
| âœ… Phase 3        | Useful commands (explain/fix/run)    | â³       |
| âœ… Phase 4        | Multi-model backend support          | â³       |
| âœ… Phase 5        | Chat mode                            | â³       |
| âœ… Phase 6        | Plugins and UI polish                | â³       |
| âœ… Phase 7        | Deployment & packaging               | â³       |

