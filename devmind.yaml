backend: "ollama" # Options: [ openai, ollama ]

openai:
  api_key: "tests"
  api_url: "https://api.openai.com/v1/chat/completions"

ollama:
  host: "http://localhost:11434/"
  model: "qwen2.5-coder"
  # model: "gemma3"
